{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmhQwOqEs3qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import rl_dbs.gym_oscillator\n",
        "import rl_dbs.gym_oscillator.envs\n",
        "from TD3 import TD3\n",
        "from utils import ReplayBuffer\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "def train():\n",
        "    ######### Hyperparameters #########\n",
        "    random_seed = 42\n",
        "    gamma = 0.99                # discount for future rewards\n",
        "    batch_size = 64             # num of transitions sampled from replay buffer\n",
        "    lr = 0.002\n",
        "    exploration_noise = 0.1\n",
        "    polyak = 0.995              # target policy update parameter (1-tau)\n",
        "    policy_noise = 0.2          # target policy smoothing noise\n",
        "    noise_clip = 0.5\n",
        "    policy_delay = 2            # delayed policy updates parameter\n",
        "    max_episodes = 1000         # max num of episodes\n",
        "    max_timesteps = 10000       # max timesteps in one episode\n",
        "    directory = \"./td3_model/\"  # save trained models\n",
        "    filename = \"TD3_Oscillator_{}\"\n",
        "    ###################################\n",
        "\n",
        "    # Create the environment\n",
        "    env = rl_dbs.gym_oscillator.envs.oscillatorEnv()\n",
        "\n",
        "    # Get environment dimensions\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize policy and replay buffer\n",
        "    policy = TD3(lr, state_dim, action_dim, max_action)\n",
        "    replay_buffer = ReplayBuffer()\n",
        "\n",
        "    # Set random seeds\n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        env.reset(seed=random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Flexible reset function\n",
        "    def flexible_reset(env):\n",
        "        reset_result = env.reset()\n",
        "\n",
        "        # If it's a tuple, return the first element\n",
        "        if isinstance(reset_result, tuple):\n",
        "            return reset_result[0]\n",
        "\n",
        "        # If it's a numpy array, return it directly\n",
        "        if isinstance(reset_result, np.ndarray):\n",
        "            return reset_result\n",
        "\n",
        "        # If it's a list, return the first element\n",
        "        if isinstance(reset_result, list) and reset_result:\n",
        "            return reset_result[0]\n",
        "\n",
        "        # If none of the above, raise an error\n",
        "        raise ValueError(f\"Unexpected reset result type: {type(reset_result)}\")\n",
        "\n",
        "    # logging variables:\n",
        "    avg_reward = 0\n",
        "    ep_reward = 0\n",
        "    log_f = open(\"log.txt\",\"w+\")\n",
        "\n",
        "    # training procedure:\n",
        "    for episode in range(1, max_episodes+1):\n",
        "        state = flexible_reset(env)\n",
        "        ep_reward = 0\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            # select action and add exploration noise:\n",
        "            action = policy.select_action(state)\n",
        "\n",
        "            # Ensure action is in the correct format for this environment\n",
        "            action = np.array([action[0]])\n",
        "\n",
        "            # Add exploration noise\n",
        "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "            # take action in env:\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            replay_buffer.add((state, action, reward, next_state, terminated or truncated))\n",
        "            state = next_state\n",
        "\n",
        "            avg_reward += reward\n",
        "            ep_reward += reward\n",
        "\n",
        "            # Reset environment if episode is done\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            # Perform policy update if buffer has enough samples\n",
        "            if len(replay_buffer.buffer) >= batch_size:\n",
        "                policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "\n",
        "        # logging updates:\n",
        "        log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "        log_f.flush()\n",
        "\n",
        "        # print avg reward every log interval:\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = int(avg_reward / 10)\n",
        "            print(\"Episode: {}\\tAverage Reward: {}\".format(episode, avg_reward))\n",
        "            avg_reward = 0\n",
        "\n",
        "        # Save model periodically\n",
        "        if episode > 500:\n",
        "            policy.save(directory, filename.format(episode))\n",
        "\n",
        "    log_f.close()\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    }
  ]
}